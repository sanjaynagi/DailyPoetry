Excellent. Iâ€™ll write these as proper high-level engineering design documents â€” clear scope, architecture, data model, risks, and iteration plan.

â¸»

ðŸ“˜ Design Document 1

Poetry Ingestion Tool (v1 â€” PoetryDB-based)

â¸»

1. Purpose

Build a one-time + repeatable ingestion pipeline that:
	â€¢	Pulls poems from PoetryDB
	â€¢	Normalizes and cleans formatting
	â€¢	Enriches metadata
	â€¢	Stores everything in our own database
	â€¢	Removes runtime dependency on PoetryDB

This tool is not part of the production request path.
It is an internal data acquisition + curation system.

â¸»

2. Goals

Functional
	â€¢	Import full corpus from PoetryDB
	â€¢	De-duplicate poems
	â€¢	Normalize author names
	â€¢	Preserve stanza formatting exactly
	â€¢	Attach canonical author records
	â€¢	Allow re-runs safely (idempotent)

Non-Goals
	â€¢	No end-user UI
	â€¢	No direct exposure to app users
	â€¢	No real-time PoetryDB proxying

â¸»

3. System Overview

High-Level Flow

PoetryDB API
    â†“
Ingestion Script (ETL)
    â†“
Normalization + Cleaning
    â†“
Metadata Enrichment (Wikipedia/Wikidata)
    â†“
Postgres Database


â¸»

4. Architecture

Components

1ï¸âƒ£ Fetcher
	â€¢	Pull list of authors from /author
	â€¢	For each author:
	â€¢	Fetch all poems
	â€¢	Respect rate limits
	â€¢	Retry with exponential backoff

2ï¸âƒ£ Normalizer
Transforms PoetryDB JSON into internal schema.

Key transformations:
	â€¢	Join lines array into canonical text with \n
	â€¢	Preserve stanza breaks
	â€¢	Normalize whitespace
	â€¢	Strip trailing blank lines
	â€¢	Standardize quotation marks (optional)

3ï¸âƒ£ De-duplication Engine
PoetryDB sometimes contains:
	â€¢	Duplicate titles
	â€¢	Variant titles
	â€¢	Duplicate poems across formatting

Strategy:
	â€¢	Compute content hash of poem text
	â€¢	Unique index on hash
	â€¢	Log collisions

4ï¸âƒ£ Metadata Enricher
For each author:
	â€¢	Query Wikipedia API
	â€¢	Extract:
	â€¢	Short description
	â€¢	Birth/death years
	â€¢	Image URL
	â€¢	Store Wikidata ID for future refresh

Caching required.

5ï¸âƒ£ Validator
Reject poems if:
	â€¢	N lines (e.g., 200 â€” avoid epics initially)
	â€¢	Missing author
	â€¢	Missing text
	â€¢	Suspicious formatting

â¸»

5. Database Schema (Ingestion Side)

authors

id (uuid)
name (text, unique)
birth_year (int, nullable)
death_year (int, nullable)
wikipedia_url (text)
wikidata_id (text)
bio_short (text)
image_url (text)
created_at
updated_at

poems

id (uuid)
title (text)
author_id (fk)
text (text)
linecount (int)
content_hash (text, unique)
source (text)  // e.g., "poetrydb"
license (text) // "public domain"
created_at
updated_at

Indexes:
	â€¢	unique(content_hash)
	â€¢	index(author_id)
	â€¢	index(linecount)

â¸»

6. Ingestion Strategy

Phase 1 â€” Bulk Snapshot
	1.	Fetch /author
	2.	For each author:
	â€¢	Fetch all poems
	3.	Store raw JSON locally
	4.	Run normalization pass
	5.	Insert into DB

Phase 2 â€” Cleanup Pass
	â€¢	Remove poems > 120 lines (initial filter)
	â€¢	Remove malformed entries
	â€¢	Manual review sampling

â¸»

7. Idempotency & Re-runs

Key principle:

Ingestion must be safe to run multiple times.

Mechanisms:
	â€¢	Unique constraint on content_hash
	â€¢	Upsert authors on name
	â€¢	Log changes rather than overwrite silently

â¸»

8. Operational Considerations

Rate Limiting

Throttle requests to:
	â€¢	1â€“5 req/sec
	â€¢	Configurable

Logging

Log:
	â€¢	Fetch failures
	â€¢	Parse errors
	â€¢	Duplicate collisions
	â€¢	Wikipedia misses

â¸»

9. Editorial Layer (Critical)

We should not auto-publish everything.

After ingestion:

Create an editorial_status column in poems:
	â€¢	pending_review
	â€¢	approved
	â€¢	rejected

Only approved poems can be selected for daily use.

This prevents:
	â€¢	Overly long poems
	â€¢	Weak content
	â€¢	Formatting errors

â¸»

10. Output of Ingestion Tool

After v1 ingestion:
	â€¢	~1000â€“3000 poems (estimate)
	â€¢	Clean, normalized, curated dataset
	â€¢	Fully independent of PoetryDB

â¸»

11. Risks

Risk	Mitigation
PoetryDB changes	Snapshot early
Copyright uncertainty	Restrict to clearly public-domain authors
Poor formatting	Manual review layer
English-only bias	Add other sources later


â¸»

12. Future Extensions
	â€¢	Add Wikisource ingestion
	â€¢	Add Gutenberg bulk parser
	â€¢	Add tagging classifier (theme detection via NLP)
	â€¢	Add reading-time estimation
